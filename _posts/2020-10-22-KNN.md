---
layout: post
title: What’s the KNN?
subtitle: Understanding the Lazy Learner Algorithm by creating it from scratch.
gh-repo: github.com/jiobu1/ CS_Build_Week_1
#gh-badge: [star, fork, follow]
tags: [KNN, Machine Learning, Data Science]
comments: true
---
<style> h5 {text-align: center;} </style>
<img src= "../img/unit5/knn_3.png" alt="Atlanta" class="center">
<h5>Decision boundary of nearest neighbors decision on Iris dataset with 3 nearest neighbors</h5>

<h2>What is KNN Algorithm?</h2>
<p>K-Nearest Neighbor (KNN)is a non-parametric supervised machine learning algorithm. (Supervised machine learning means that the machine learns to map an input to an output based on labeled training data.) It is one of the simplest algorithms used in machine learning for regression and classification. KNN follows the “birds of a feather” strategy in determining where the new data fits. KNN uses all available data and classifies the new data or case based on a similarity measure, or distance function. The new data is then assigned to the class to which most neighbors belong to. </p>

<h2>Why is KNN called a “Lazy Learner”?</h2>
<p>KNN is often referred to as a <b>lazy learner</b>. This means that the algorithm does not use the training data points to do any generalizations. In other words, there is no explicit training phase. Lack of generalization means that KNN keeps all the training data. It is a <b>non-parametric learning algorithm</b> because it doesn’t assume anything about the underlying data.</p>

<h2>How to make predictions using KNN algorithm?</h2>
<p>To classify an unknown instance represented by some feature vectors as a point in the feature space, the KNN classifier calculates the distances between the point and points in the training data set. Usually, the <b>Euclidean distance</b>is used as the distance metric.</p>

<img src = "../img/unit5/1*jyZpd_nenl5vPfCEgZtEiw.png" alt="Distance" class="center">
<h5>Find the mode of the labels the new data is closet to.</h5>

<h4>Distance Formulas</h4>
<p>Below are other distance formulas used in the KNN algorithm.</p>
<ul>
    <li><b>Minkowski distance</b> is a normed vector space that can be considered as a generalization of the Euclidean distance and the Manhattan distance.</li>
    <img src="../img/unit5/below_is_the_formula_for_minkowski_distance.png" alt="Minkowski" class="center">
    <h5>Minkowski distance</h5>
    <li><b>Euclidean distance</b> is the most commonly used distance formula when calculating KNN. Euclidean distance formula finds the shortest distance between points A and B. This is sometimes referred to as the Pythagorean Theorem.</li>
    <img src="../img/unit5/euclidean_distance_to_measure_the_similarity_between_observations.png" alt="Euclidean" class="center">
    <h5>Euclidean distance</h5>
    <li>Another distance formula is the <b>Manhattan distance</b> formula. This formula takes the sum of the absolute values of the differences of the coordinates.</li>
    <img src="../img/unit5/below_is_the_formula_for_manhattan_distance.png" alt="Manhattan" class="center">
    <h5>Manhattan distance</h5>
    <li>Other formulas include Chebyshev, Wminkowski, Seuclidean, Mahalanobis.</li>
</ul>

<p>You can read this article about the importance of distance formula in calculating KNN. https://arxiv.org/pdf/1708.04321.pdf)</p>

<h2>How to choose the value of k?</h2>
<p>The “K” in KNN stands for the number of nearest neighbors the model is using to figure out how to assign a class for your new data. This is an important parameter for achieving better accuracy of your model. While there is no structured method to find the best value for K, here are some suggestions.</p>

<ol>
    <li>Take the square root of N, where N is the number of samples in your training data (k=sqrt(N))</l1>
    <li>Choose an odd number. This helps to avoid confusion between two classes of data.</l1>
    <li>Cross-Validation. Use a small subset of the training data, called the validation set, and test out different possible values of K. Then choose the K that yields the best results for the validation set.</l1>
</ol>

<h2>Implementing the KNN Algorithm</h2>
<p>The KNN model can be implemented in 4 easy steps. To demonstrate the KNN algorithm, I am using it on the iris dataset. This is a preloaded dataset from the sklearn library. The iris dataset includes three iris species — Setosa, Versicolor, Virginica, with 50 samples from each. For each sample, we have sepal length, width, and petal length, and width.</p>

<img src="../img/unit5/iris-machinelearning.png" alt="Iris" class="center">
<h5><a href="https://www.datacamp.com/community/tutorials/machine-learning-in-r"> Machine Learning in R for beginners</a></h5>

<p><b>1. Look at the Data</b></p>

<p>The KNN algorithm will use the relationships between the 4 features and to make predictions on the test data.</p>

<p><img src="../img/unit5/2D_scatter.png" alt="2D Scatterplot of Iris" class="center"></p>
<p><img src="../img/unit5/boxplot.png" alt="Boxplot" class="center"></p>
<h5>Boxplot of Iris Features</h5>

<pre>
<code>
    # Load Data
    # Import library
    from sklearn.datasets import load_iris

    iris = load_iris()

    # Separate into target and features
    # Scale features
    X = scale(iris.data)
    y = iris.target
</code>
</pre>